{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#蘋果日報\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import json\n",
    "import time,random,datetime,re\n",
    "from tomorrow import threads\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搜尋網址基本連結\n",
    "ua = UserAgent()\n",
    "headers = {\"User-Agent\":ua.random}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllLinks():\n",
    "    \n",
    "    links = []\n",
    "    page = 1\n",
    "    domin = \"https://tw.news.appledaily.com\"\n",
    "    while True :\n",
    "        try:\n",
    "            page_url = \"https://tw.news.appledaily.com/politics/realtime/\" + str(page) \n",
    "            res = requests.get(page_url, headers = headers)\n",
    "            soup = BeautifulSoup(res.text,\"html.parser\")\n",
    "            for i in soup.find('div',{'class':'abdominis rlby clearmen'}).find_all(\"a\",target=\"_blank\"):\n",
    "                links.append(domin + i.get('href')) \n",
    "            print(\"現在頁數：\",page)\n",
    "            page += 1\n",
    "\n",
    "        except:\n",
    "            break\n",
    "            print(f'總頁數:{page}')\n",
    "        time.sleep(random.randint(3,8))\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬內文\n",
    "\n",
    "def AppleCrawler(response):\n",
    "\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    Url = soup.find('link',{'rel':'canonical'})['href']\n",
    "    Title = soup.find('article',class_='ndArticle_leftColumn').find('h1').text.replace('\\u3000','')\n",
    "    CreateTime = datetime.strptime(soup.find('div',{'class':'ndArticle_creat'}).text.replace('出版時間：',''),'%Y/%m/%d %H:%M')\n",
    "    content_list = []\n",
    "    any(content_list.append(i.text) for i in soup.find('div',class_='ndArticle_margin').find_all('p'))\n",
    "    Content = \"\".join(content_list)\n",
    "    Tag = soup.find('meta',{'name':'keywords'})['content'].split(',')\n",
    "\n",
    "    ndf = pd.DataFrame(data = [{'Source':'News',\n",
    "                                'Site':'蘋果日報',\n",
    "                                'Url':Url,\n",
    "                                'Title':Title,\n",
    "                                'CreateTime':CreateTime,\n",
    "                                'Author':'無作者',\n",
    "                                'Content':Content,\n",
    "                                'Tag':Tag}],\n",
    "                                columns = ['Source','Site','Url','CreateTime','Author','Title','Content','Tag'])\n",
    "    time.sleep(random.randint(5,8))\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Site</th>\n",
       "      <th>Url</th>\n",
       "      <th>CreateTime</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>蘋果日報</td>\n",
       "      <td>https://tw.appledaily.com/new/realtime/2019112...</td>\n",
       "      <td>2019-11-23 10:45:00</td>\n",
       "      <td>無作者</td>\n",
       "      <td>李佳芬發燒缺席汪志冰造勢李四川代打站台</td>\n",
       "      <td>國民黨總統參選人韓國瑜的太太李佳芬原預定今（23日）出席黨內立委參選人汪志冰競選造勢活動，卻...</td>\n",
       "      <td>[韓國瑜, 李佳芬, 高美蘭]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source  Site                                                Url  \\\n",
       "0   News  蘋果日報  https://tw.appledaily.com/new/realtime/2019112...   \n",
       "\n",
       "           CreateTime Author                Title  \\\n",
       "0 2019-11-23 10:45:00    無作者  李佳芬發燒缺席汪志冰造勢李四川代打站台   \n",
       "\n",
       "                                             Content              Tag  \n",
       "0  國民黨總統參選人韓國瑜的太太李佳芬原預定今（23日）出席黨內立委參選人汪志冰競選造勢活動，卻...  [韓國瑜, 李佳芬, 高美蘭]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#測試抓取一篇文章並存為是否成功\n",
    "\n",
    "url = \"https://tw.news.appledaily.com/politics/realtime/20191123/1667642/\"\n",
    "res = requests.get(url,headers=headers)\n",
    "AppleCrawler(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多線程爬蟲定義\n",
    "@threads(100)\n",
    "def MultiThread_Crawl(url):\n",
    "    try:\n",
    "        return requests.get(url, headers=headers)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlAllNews(links):\n",
    "    # 爬取全部連結的新聞內容\n",
    "    print('There are {} links in pages.'.format(len(links)))\n",
    "    \n",
    "    # 多線程爬蟲\n",
    "    responses = [MultiThread_Crawl(link) for link in links]\n",
    "\n",
    "    # 整理成DataFrame\n",
    "    allData = []\n",
    "    for response in responses:\n",
    "        try:\n",
    "            ndf = AppleCrawler(response)\n",
    "            allData.append(ndf)\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.concat(allData, ignore_index=True)\n",
    "    print('There are {} News in DataFrame.'.format(len(df)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在頁數： 1\n",
      "現在頁數： 2\n",
      "現在頁數： 3\n",
      "現在頁數： 4\n",
      "現在頁數： 5\n",
      "現在頁數： 6\n",
      "現在頁數： 7\n",
      "現在頁數： 8\n",
      "現在頁數： 9\n",
      "現在頁數： 10\n",
      "現在頁數： 11\n",
      "現在頁數： 12\n",
      "現在頁數： 13\n",
      "現在頁數： 14\n",
      "現在頁數： 15\n",
      "There are 421 links in pages.\n",
      "There are 79 News in DataFrame.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "links = GetAllLinks() \n",
    "df = CrawlAllNews(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_mongo_test(select_df):\n",
    "    user_name = \"test\"\n",
    "    password = \"pn123456\"\n",
    "    connect_IP = \"ntcpn108.ddns.net\"\n",
    "    DB_name = \"test\"\n",
    "    mode = \"SCRAM-SHA-1\"\n",
    "    \n",
    "    url = \"mongodb://\"+str(user_name)+\":\"+str(password)+\"@\"+str(connect_IP)+\":27017/?authSource=\"+str(DB_name)+\"&authMechanism=\"+str(mode)\n",
    "    client = MongoClient(url)\n",
    "    db = client.test   #DB_name\n",
    "    collection = db.test_news  #collection_name\n",
    "    \n",
    "    try:\n",
    "        print(\"total \"+str(len(select_df))+\" into database\")\n",
    "        records = select_df.to_dict('records')\n",
    "        collection.insert_many(records,ordered=False)\n",
    "\n",
    "    except pymongo.errors.BulkWriteError as e:\n",
    "        print(\"already \"+str(len(e.details['writeErrors']))+\" duplicated wont into database\")\n",
    "        print(str(len(select_df) - len(e.details['writeErrors']))+\" into database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_mongo(select_df):\n",
    "    user_name = \"pol\"\n",
    "    password = \"pn123456\"\n",
    "    connect_IP = \"192.168.1.126\"\n",
    "    DB_name = \"political\"\n",
    "    mode = \"SCRAM-SHA-1\"\n",
    "    \n",
    "    url = \"mongodb://\"+str(user_name)+\":\"+str(password)+\"@\"+str(connect_IP)+\":27017/?authSource=\"+str(DB_name)+\"&authMechanism=\"+str(mode)\n",
    "    client = MongoClient(url)\n",
    "    db = client.political   #DB_name\n",
    "    collection = db.news_pol  #collection_name\n",
    "    \n",
    "    try:\n",
    "        print(\"total \"+str(len(select_df))+\" into database\")\n",
    "        records = select_df.to_dict('records')\n",
    "        collection.insert_many(records,ordered=False)\n",
    "\n",
    "    except pymongo.errors.BulkWriteError as e:\n",
    "        print(\"already \"+str(len(e.details['writeErrors']))+\" duplicated wont into database\")\n",
    "        print(str(len(select_df) - len(e.details['writeErrors']))+\" into database\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在頁數： 1\n",
      "現在頁數： 2\n",
      "現在頁數： 3\n",
      "現在頁數： 4\n",
      "現在頁數： 5\n",
      "現在頁數： 6\n",
      "現在頁數： 7\n",
      "現在頁數： 8\n",
      "現在頁數： 9\n",
      "現在頁數： 10\n",
      "現在頁數： 11\n",
      "現在頁數： 12\n",
      "現在頁數： 13\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "links = GetAllLinks()\n",
    "\n",
    "allData = []\n",
    "seccess_count = 0\n",
    "error_count = 0\n",
    "for link in links:\n",
    "    try:\n",
    "        res = requests.get(link,headers=headers)\n",
    "        ndf = AppleCrawler(res)\n",
    "        allData.append(ndf)\n",
    "        seccess_count += 1\n",
    "    \n",
    "    except:\n",
    "        error_count += 1\n",
    "        pass\n",
    "df = pd.concat(allData, ignore_index=True)\n",
    "dataframe_to_mongo_test(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
