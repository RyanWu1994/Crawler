{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTT\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from tomorrow import threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "headers = {\"User-Agent\":ua.random}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TheTime():\n",
    "    \"\"\"抓本機系統時間\"\"\"\n",
    "    ISOTIMEFORMAT = '%Y'\n",
    "    theTime = datetime.now().strftime(ISOTIMEFORMAT)\n",
    "    return theTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LastPage(board):\n",
    "    '''抓最後一頁'''\n",
    "    res = requests.get(\"https://www.ptt.cc/bbs/\"+str(board)+\"/index.html\",headers=headers,cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    pages = soup.find_all(\"div\",class_=\"btn-group btn-group-paging\")[0].find_all(\"a\",class_=\"btn wide\")[1].get(\"href\").strip(\"/bbs/\"+str(board)+\"/index .html\")\n",
    "    return int(pages)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllLinks(board,lastpage,stop_page):\n",
    "    '''抓全部link,輸入的lastpage必須是int'''\n",
    "    links = []\n",
    "    while True:\n",
    "        res = requests.get(\"https://www.ptt.cc/bbs/\"+str(board)+\"/index\"+str(lastpage)+\".html\",headers=headers,cookies={'over18': '1'})\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        domain = \"https://www.ptt.cc/\"\n",
    "        for i in soup.find_all(\"div\",class_=\"title\"):\n",
    "            try:\n",
    "                links.append(domain + i.find(\"a\").get(\"href\"))\n",
    "            except:\n",
    "                pass\n",
    "        lastpage -= 1\n",
    "        if lastpage == int(stop_page):\n",
    "            break\n",
    "    return links\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "def GetSearchLinks(board,search,stop_page=10000):\n",
    "    '''search是要搜尋的關鍵字 '''\n",
    "    first_page = 1\n",
    "    links = []\n",
    "    while True:\n",
    "        res = requests.get(\"https://www.ptt.cc/bbs/\"+str(board)+\"/search?page=\"+str(first_page)+\"&q=\"+str(search),headers=headers,cookies={'over18': '1'})\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        try:\n",
    "            if soup.find(\"div\",class_=\"bbs-screen bbs-content\").text == '404 - Not Found.':\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        domain = \"https://www.ptt.cc/\"\n",
    "        for i in soup.find_all(\"div\",class_=\"title\"):\n",
    "            try:\n",
    "                links.append(domain + i.find(\"a\").get(\"href\"))\n",
    "            except:\n",
    "                pass\n",
    "        first_page += 1\n",
    "        \n",
    "        if first_page > int(stop_page):\n",
    "            break\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPTT(response):\n",
    "    '''板、url、發文日期、作者、title、內容、回文'''\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    Board = soup.find(\"div\",class_=\"article-metaline-right\").find(\"span\",class_=\"article-meta-value\").text\n",
    "    Url = soup.find(\"link\",rel=\"canonical\").get(\"href\")\n",
    "    CreateTime = soup.find_all(\"div\",class_=\"article-metaline\")[2].find(\"span\",class_=\"article-meta-value\").text\n",
    "    CreateTime = datetime.strptime(CreateTime,'%a %b %d %H:%M:%S %Y')\n",
    "    Author = soup.find_all(\"div\",class_=\"article-metaline\")[0].find(\"span\",class_=\"article-meta-value\").text\n",
    "    Title = soup.find_all(\"div\",class_=\"article-metaline\")[1].find(\"span\",class_=\"article-meta-value\").text\n",
    "\n",
    "    content_list = []\n",
    "    for i in soup.find_all(\"div\",class_=\"bbs-content\")[2].contents[4:]:\n",
    "        if str(i)[:23] == '<span class=\"f2\">※ 發信站:':\n",
    "            break\n",
    "        else:\n",
    "            try:    \n",
    "                content_list.append(i.text.replace(\"\\n\",\" \"))\n",
    "            except:\n",
    "                content_list.append(i.replace(\"\\n\",\" \"))\n",
    "    Content = \" \".join(content_list)\n",
    "\n",
    "    Reply = []\n",
    "    for i in range(len(soup.find_all(\"div\",class_=\"push\"))):\n",
    "        Reaction  = soup.find_all(\"div\",class_=\"push\")[i].contents[0].text.strip(\" \")\n",
    "        Name = soup.find_all(\"div\",class_=\"push\")[i].contents[1].text.strip(\" \")\n",
    "        Comment = soup.find_all(\"div\",class_=\"push\")[i].contents[2].text.strip(\": \")\n",
    "        Reply_dict = {\"Name\":Name,\"Comment\":Comment,\"Reaction\":Reaction}\n",
    "        Reply.append(Reply_dict)\n",
    "\n",
    "    ndf = pd.DataFrame(data = [{\n",
    "                                \"Source\":\"PTT\",\n",
    "                                \"Site\":Board,\n",
    "                                \"Url\":Url,\n",
    "                                \"CreateTime\":CreateTime,\n",
    "                                \"Author\":Author,\n",
    "                                \"Title\":Title,\n",
    "                                \"Content\":Content,\n",
    "                                \"Reply\":Reply,\n",
    "                                \"ReplyCount\":len(Reply)}],\n",
    "                       columns = [\"Source\", \"Site\", \"Url\", \"CreateTime\", \"Author\", \"Title\", \"Content\",\"Reply\",\"ReplyCount\"]) \n",
    "    \n",
    "    return ndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/M.1575128097.A.D41.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Site</th>\n",
       "      <th>Url</th>\n",
       "      <th>CreateTime</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Reply</th>\n",
       "      <th>ReplyCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PTT</td>\n",
       "      <td>Gossiping</td>\n",
       "      <td>https://www.ptt.cc/bbs/Gossiping/M.1575128097....</td>\n",
       "      <td>2019-11-30 23:34:55</td>\n",
       "      <td>dilson (中肯的話通常不好聽懂嗎)</td>\n",
       "      <td>Re: [問卦] 如果被侵略 要打仗 大家願意上戰場嗎</td>\n",
       "      <td>要人民上戰場保護家園  前提是政客說的保護家園 這個家園有讓人民好過嗎?  若薪水低  物...</td>\n",
       "      <td>[{'Name': 'philip2364', 'Comment': '言論自由不代表你可以...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source       Site                                                Url  \\\n",
       "0    PTT  Gossiping  https://www.ptt.cc/bbs/Gossiping/M.1575128097....   \n",
       "\n",
       "           CreateTime                Author                        Title  \\\n",
       "0 2019-11-30 23:34:55  dilson (中肯的話通常不好聽懂嗎)  Re: [問卦] 如果被侵略 要打仗 大家願意上戰場嗎   \n",
       "\n",
       "                                             Content  \\\n",
       "0   要人民上戰場保護家園  前提是政客說的保護家園 這個家園有讓人民好過嗎?  若薪水低  物...   \n",
       "\n",
       "                                               Reply  ReplyCount  \n",
       "0  [{'Name': 'philip2364', 'Comment': '言論自由不代表你可以...          15  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# board = \"Gossiping\"\n",
    "# url = 'https://www.ptt.cc/bbs/Gossiping/M.1575128097.A.D41.html'\n",
    "# res = requests.get(url,headers=headers,cookies={'over18': '1'})\n",
    "# GetPTT(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多線程爬蟲定義\n",
    "@threads(100)\n",
    "def MultiThread_Crawl(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers,cookies={'over18': '1'})\n",
    "        return res\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlAllNews(links):\n",
    "    # 爬取全部連結的新聞內容\n",
    "    print('There are {} links in pages.'.format(len(links)))\n",
    "    \n",
    "    # 多線程爬蟲\n",
    "    responses = [MultiThread_Crawl(link) for link in links]\n",
    "\n",
    "    # 整理成DataFrame\n",
    "    allData = []\n",
    "    for response in responses:\n",
    "        try:\n",
    "            ndf = GetPTT(response)\n",
    "            allData.append(ndf)           \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = pd.concat(allData, ignore_index=True)\n",
    "    print('There are {} News in DataFrame.'.format(len(df)))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "def dataframe_to_mongo_test(select_df):\n",
    "    user_name = \"test\"\n",
    "    password = \"pn123456\"\n",
    "    connect_IP = \"ntcpn108.ddns.net\"\n",
    "    DB_name = \"test\"\n",
    "    mode = \"SCRAM-SHA-1\"\n",
    "    \n",
    "    url = \"mongodb://\"+str(user_name)+\":\"+str(password)+\"@\"+str(connect_IP)+\":27017/?authSource=\"+str(DB_name)+\"&authMechanism=\"+str(mode)\n",
    "    client = MongoClient(url)\n",
    "    db = client.test   #DB_name\n",
    "    collection = db.test_forum_pol  #collection_name\n",
    "    \n",
    "    try:\n",
    "        print(\"total \"+str(len(select_df))+\" into database\")\n",
    "        records = select_df.to_dict('records')\n",
    "        collection.insert_many(records,ordered=False)\n",
    "\n",
    "    except pymongo.errors.BulkWriteError as e:\n",
    "        print(\"already \"+str(len(e.details['writeErrors']))+\" duplicated wont into database\")\n",
    "        print(str(len(select_df) - len(e.details['writeErrors']))+\" into database\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 政治\n",
    "def dataframe_to_mongo(select_df):\n",
    "    user_name = \"pol\"\n",
    "    password = \"pn123456\"\n",
    "    connect_IP = \"192.168.1.126\"\n",
    "    DB_name = \"political\"\n",
    "    mode = \"SCRAM-SHA-1\"\n",
    "    \n",
    "    url = \"mongodb://\"+str(user_name)+\":\"+str(password)+\"@\"+str(connect_IP)+\":27017/?authSource=\"+str(DB_name)+\"&authMechanism=\"+str(mode)\n",
    "    client = MongoClient(url)\n",
    "    db = client.political   #DB_name\n",
    "    collection = db.forum_pol  #collection_name\n",
    "    \n",
    "    try:\n",
    "        print(\"total \"+str(len(select_df))+\" into database\")\n",
    "        records = select_df.to_dict('records')\n",
    "        collection.insert_many(records,ordered=False)\n",
    "\n",
    "    except pymongo.errors.BulkWriteError as e:\n",
    "        print(\"already \"+str(len(e.details['writeErrors']))+\" duplicated wont into database\")\n",
    "        print(str(len(select_df) - len(e.details['writeErrors']))+\" into database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"立委\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"立法委員\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"大選\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"選舉\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"2020\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = GetSearchLinks(\"Gossiping\",search=\"區域\",stop_page=20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastpage=LastPage(\"politics\")\n",
    "links = GetAllLinks(board=\"politics\",lastpage=lastpage,stop_page=lastpage-20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastpage=LastPage(\"HatePolitics\")\n",
    "links = GetAllLinks(board=\"HatePolitics\",lastpage=lastpage,stop_page=lastpage-20)\n",
    "df = CrawlAllNews(links)\n",
    "dataframe_to_mongo(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
